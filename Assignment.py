# -*- coding: utf-8 -*-
"""M24CSA013_M24CSA032_M24CSA036_M24CSA038.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RLdNO6GJrD7VlnCMw_jvJ7Lf8KQkXN6i
"""

!pip uninstall -y torch torchtext torchvision torchaudio
!pip install torch==2.1.0 torchtext==0.16.0
!pip install sacrebleu
!pip install gensim==4.3.3 numpy==1.24.3
!unzip MT.zip
!pip install indic-nlp-library

import gensim.downloader as api
print(list(api.info()['models'].keys()))

"""#Hindi_vectors"""

!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz
!gunzip cc.hi.300.vec.gz

word2vec = api.load("word2vec-google-news-300")
from gensim.models import KeyedVectors
hindi_vectors = KeyedVectors.load_word2vec_format("cc.hi.300.vec", binary=False)
PRETRAINED_HINDI_FASTTEXT = "fasttext-wiki-hindi-300"

!pip install torch==2.1.0 torchtext==0.16.0
!pip install sacrebleu
!pip install gensim==4.3.3 numpy==1.24.3

!pip install indic-nlp-library

"""# Libraries"""

import re
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from collections import Counter
import sacrebleu
import gensim.downloader as api
import string
from torch.utils.data import Dataset, DataLoader
from torchtext.vocab import build_vocab_from_iterator
from torchtext.data.utils import get_tokenizer
from indicnlp.tokenize import indic_tokenize
from torch.nn.utils.rnn import pad_sequence
from sklearn.model_selection import KFold

"""# Hyperparameters"""

"""Embedding dimension"""
LSTM_EMB_DIM = 300

"""Hidden dimension for decoder; encoder is bidirectional so its outputs are 2*LSTM_HID_DIM"""
LSTM_HID_DIM = 512

"""Number of layers for both encoder and decoder"""
LSTM_N_LAYERS = 2

"""Dropout rate"""
LSTM_DROPOUT = 0.5


BATCH_SIZE = 64
NUM_EPOCHS = 10

"""Gradient clipping"""
CLIP = 1.0
LR = 0.001

"""L2 regularization weight decay"""
WEIGHT_DECAY = 1e-5

SPECIAL_TOKENS = ['<unk>', '<pad>', '<sos>', '<eos>']

"""Pre-trained embedding dimension"""
EMBEDDING_DIM = 300

DATA_FRACTION = 0.70
NUM_FOLDS = 5

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

"""# For LSTM

# Data Loading & Preprocessing
"""

def load_data(eng_file, hindi_file):
    with open(eng_file, 'r', encoding='utf-8') as f:
        eng_sentences = f.readlines()
    with open(hindi_file, 'r', encoding='utf-8') as f:
        hindi_sentences = f.readlines()
    eng_sentences = [s.strip() for s in eng_sentences]
    hindi_sentences = [s.strip() for s in hindi_sentences]
    data_size = int(len(eng_sentences) * DATA_FRACTION)
    return eng_sentences[:data_size], hindi_sentences[:data_size]

eng_train, hindi_train = load_data('MT/english.train.txt', 'MT/hindi.train.txt')
eng_test, hindi_test = load_data('MT/english.test.txt', 'MT/hindi.test.txt')

eng_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')
def tokenize_hindi(text):
    return indic_tokenize.trivial_tokenize_indic(text)
def tokenize(sentences, tokenizer):
    return [['<sos>'] + [tok.lower() for tok in tokenizer(sent)] + ['<eos>'] for sent in sentences]
def clean_hindi_tokenizer(text):
    tokens = indic_tokenize.trivial_tokenize_indic(text)
    return [t.lower() for t in tokens if t not in string.punctuation and not re.fullmatch(r'\W+', t)]

eng_train_tokens = tokenize(eng_train, eng_tokenizer)
hindi_train_tokens = tokenize(hindi_train, clean_hindi_tokenizer)
eng_test_tokens = tokenize(eng_test, eng_tokenizer)
hindi_test_tokens = tokenize(hindi_test, clean_hindi_tokenizer)

counts = Counter([w for s in hindi_train_tokens for w in s])
print(counts.most_common(20))

"""# Building Vocabularies"""

eng_vocab = build_vocab_from_iterator(eng_train_tokens, specials=SPECIAL_TOKENS, min_freq=0)
hindi_vocab = build_vocab_from_iterator(hindi_train_tokens, specials=SPECIAL_TOKENS, min_freq=0)
eng_vocab.set_default_index(eng_vocab['<unk>'])
hindi_vocab.set_default_index(hindi_vocab['<unk>'])

"""# Embedding Matrices"""

def create_embedding_matrix(vocab, vectors, embedding_dim=300):
    embedding_matrix = np.zeros((len(vocab), embedding_dim))
    oov_count = 0
    for word, idx in vocab.get_stoi().items():
        try:
            embedding_matrix[idx] = vectors[word]
        except KeyError:
            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))
            oov_count += 1
    print(f"OOV tokens: {oov_count}/{len(vocab)}")
    return torch.from_numpy(embedding_matrix).float()

eng_embedding_matrix = create_embedding_matrix(eng_vocab, word2vec, EMBEDDING_DIM)
hindi_embedding_matrix = create_embedding_matrix(hindi_vocab, hindi_vectors, EMBEDDING_DIM)

"""# Prepare DataLoaders"""

class TranslationDataset(Dataset):
    def __init__(self, src_data, tgt_data, src_vocab, tgt_vocab):
        self.src_data = src_data
        self.tgt_data = tgt_data
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
    def __len__(self):
        return len(self.src_data)
    def __getitem__(self, idx):
        src_item = torch.tensor([self.src_vocab[token] for token in self.src_data[idx]])
        tgt_item = torch.tensor([self.tgt_vocab[token] for token in self.tgt_data[idx]])
        return src_item, tgt_item

def collate_fn(batch):
    src_batch, tgt_batch = zip(*batch)
    src_batch = pad_sequence(src_batch, padding_value=eng_vocab['<pad>'], batch_first=True)
    tgt_batch = pad_sequence(tgt_batch, padding_value=hindi_vocab['<pad>'], batch_first=True)
    return src_batch, tgt_batch

"""# K-Fold split for training/validation using training data"""

kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)
train_idx, val_idx = list(kf.split(eng_train))[0]
eng_fold_train = [eng_train[i] for i in train_idx]
hindi_fold_train = [hindi_train[i] for i in train_idx]
eng_fold_val = [eng_train[i] for i in val_idx]
hindi_fold_val = [hindi_train[i] for i in val_idx]

eng_fold_train_tokens = tokenize(eng_fold_train, eng_tokenizer)
hindi_fold_train_tokens = tokenize(hindi_fold_train, clean_hindi_tokenizer)
eng_fold_val_tokens = tokenize(eng_fold_val, eng_tokenizer)
hindi_fold_val_tokens = tokenize(hindi_fold_val, clean_hindi_tokenizer)

train_dataset = TranslationDataset(eng_fold_train_tokens, hindi_fold_train_tokens, eng_vocab, hindi_vocab)
val_dataset = TranslationDataset(eng_fold_val_tokens, hindi_fold_val_tokens, eng_vocab, hindi_vocab)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)

"""# Gold test DataLoader"""

gold_test_dataset = TranslationDataset(eng_test_tokens, hindi_test_tokens, eng_vocab, hindi_vocab)
gold_test_loader = DataLoader(gold_test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)

"""# Model Definitions with Attention"""

class BiLSTMEncoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding.from_pretrained(eng_embedding_matrix, freeze=False)
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, bidirectional=True, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        self.fc_hidden = nn.Linear(hid_dim * 2, hid_dim)
        self.fc_cell = nn.Linear(hid_dim * 2, hid_dim)
    def forward(self, src):
        embedded = self.dropout(self.embedding(src))
        outputs, (hidden, cell) = self.rnn(embedded)
        hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
        cell_cat = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1)
        hidden_final = torch.tanh(self.fc_hidden(hidden_cat))
        cell_final = torch.tanh(self.fc_cell(cell_cat))
        n_layers = hidden.size(0) // 2
        hidden_init = hidden_final.unsqueeze(0).repeat(n_layers, 1, 1)
        cell_init = cell_final.unsqueeze(0).repeat(n_layers, 1, 1)
        return outputs, hidden_init, cell_init

class BahdanauAttention(nn.Module):
    def __init__(self, enc_hid_dim, dec_hid_dim):
        super().__init__()
        self.attn = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)
        self.v = nn.Linear(dec_hid_dim, 1, bias=False)
    def forward(self, hidden, encoder_outputs):
        src_len = encoder_outputs.shape[1]
        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))
        attention = self.v(energy).squeeze(2)
        return F.softmax(attention, dim=1)

class AttnDecoder(nn.Module):
    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):
        super().__init__()
        self.output_dim = output_dim
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.attention = attention
        self.rnn = nn.LSTM(emb_dim + enc_hid_dim, dec_hid_dim, n_layers, dropout=dropout, batch_first=True)
        self.fc_out = nn.Linear(dec_hid_dim + enc_hid_dim + emb_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
    def forward(self, input, hidden, cell, encoder_outputs):
        """[B, 1]"""
        input = input.unsqueeze(1)
        """[B, 1, emb_dim]"""
        embedded = self.dropout(self.embedding(input))
        """[B, src_len]"""
        a = self.attention(hidden[-1], encoder_outputs)
        """[B, 1, src_len]"""
        a = a.unsqueeze(1)
        """"[B, 1, enc_hid_dim]"""
        context = torch.bmm(a, encoder_outputs)
        """[B, 1, emb_dim+enc_hid_dim]"""
        rnn_input = torch.cat((embedded, context), dim=2)
        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))
        output = output.squeeze(1)
        embedded = embedded.squeeze(1)
        context = context.squeeze(1)
        prediction = self.fc_out(torch.cat((output, context, embedded), dim=1))
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size = trg.shape[0]
        trg_len = trg.shape[1]
        trg_vocab_size = self.decoder.output_dim
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)
        encoder_outputs, hidden, cell = self.encoder(src)
        input = trg[:, 0]
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)
            outputs[:, t] = output
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[:, t] if teacher_force else top1
        return outputs

INPUT_DIM = len(eng_vocab)
OUTPUT_DIM = len(hindi_vocab)
encoder = BiLSTMEncoder(INPUT_DIM, EMBEDDING_DIM, LSTM_HID_DIM, LSTM_N_LAYERS, LSTM_DROPOUT)
attention = BahdanauAttention(enc_hid_dim=LSTM_HID_DIM*2, dec_hid_dim=LSTM_HID_DIM)
decoder = AttnDecoder(OUTPUT_DIM, EMBEDDING_DIM, enc_hid_dim=LSTM_HID_DIM*2, dec_hid_dim=LSTM_HID_DIM,
                      n_layers=LSTM_N_LAYERS, dropout=LSTM_DROPOUT, attention=attention)
model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)
print(f"Model is using device: {DEVICE}")

"""# Optimizer and Loss Function"""

optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
criterion = nn.CrossEntropyLoss(ignore_index=hindi_vocab['<pad>'])

"""# Training & Early Stopping with Visualizations"""

def train_epoch(model, iterator, optimizer, criterion, clip, epoch):
    teacher_forcing_ratio = max(0.5 * (0.99 ** epoch), 0.1)
    model.train()
    epoch_loss = 0
    for src, trg in iterator:
        src, trg = src.to(DEVICE), trg.to(DEVICE)
        optimizer.zero_grad()
        output = model(src, trg, teacher_forcing_ratio=teacher_forcing_ratio)
        seq_len = min(output.shape[1], trg.shape[1])
        output = output[:, 1:seq_len, :].contiguous().view(-1, output.shape[-1])
        trg = trg[:, 1:seq_len].contiguous().view(-1)
        loss = criterion(output, trg)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        epoch_loss += loss.item()
    return epoch_loss / len(iterator)

def evaluate_epoch(model, iterator, criterion):
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for src, trg in iterator:
            src, trg = src.to(DEVICE), trg.to(DEVICE)
            output = model(src, trg, teacher_forcing_ratio=0.0)
            seq_len = min(output.shape[1], trg.shape[1])
            output = output[:, 1:seq_len, :].contiguous().view(-1, output.shape[-1])
            trg = trg[:, 1:seq_len].contiguous().view(-1)
            loss = criterion(output, trg)
            epoch_loss += loss.item()
    return epoch_loss / len(iterator)

def compute_token_accuracy(model, iterator):
    model.eval()
    total_correct = 0
    total_tokens = 0
    with torch.no_grad():
        for src, trg in iterator:
            src, trg = src.to(DEVICE), trg.to(DEVICE)
            output = model(src, trg, teacher_forcing_ratio=0.0)
            preds = output.argmax(dim=-1)
            non_pad = trg != hindi_vocab['<pad>']
            total_correct += ((preds == trg) & non_pad).sum().item()
            total_tokens += non_pad.sum().item()
    return total_correct / total_tokens if total_tokens > 0 else 0.0

train_losses = []
val_losses = []
bleu_scores = []
token_accuracies = []

patience = 5
best_val_loss = float('inf')
patience_counter = 0
best_model_state = None

for epoch in range(NUM_EPOCHS):
    train_loss = train_epoch(model, train_loader, optimizer, criterion, CLIP, epoch)
    val_loss = evaluate_epoch(model, val_loader, criterion)
    train_losses.append(train_loss)
    val_losses.append(val_loss)

    """Evaluate BLEU on validation set (corpus-level)"""
    model.eval()
    actuals, predictions = [], []
    with torch.no_grad():
        for src, trg in val_loader:
            src, trg = src.to(DEVICE), trg.to(DEVICE)
            output = model(src, trg, teacher_forcing_ratio=0.0)
            pred_tokens = output.argmax(dim=-1).cpu().numpy()
            for i in range(len(src)):
                pred_sentence = [hindi_vocab.lookup_token(idx) for idx in pred_tokens[i] if idx != hindi_vocab['<pad>']]
                actual_sentence = [hindi_vocab.lookup_token(idx) for idx in trg[i].cpu().numpy() if idx != hindi_vocab['<pad>']]
                predictions.append(" ".join(pred_sentence))
                actuals.append([" ".join(actual_sentence)])
    bleu = sacrebleu.corpus_bleu(predictions, actuals).score
    bleu_scores.append(bleu / 100)

    """Compute token-level accuracy on validation set"""
    acc = compute_token_accuracy(model, val_loader)
    token_accuracies.append(acc)

    print(f"Epoch {epoch+1}: Train Loss={train_loss:.3f}, Val Loss={val_loss:.3f}, BLEU={bleu:.2f}, Token Acc={acc*100:.2f}%")

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_state = model.state_dict()
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("Early stopping triggered!")
            break

overall_train_loss = np.mean(train_losses)
overall_val_loss = np.mean(val_losses)
overall_avg_bleu = np.mean(bleu_scores)
overall_token_acc = np.mean(token_accuracies)

print("\nOverall Training Metrics:")
print(f"Average Train Loss: {overall_train_loss:.3f}")
print(f"Average Val Loss: {overall_val_loss:.3f}")
print(f"Average BLEU Score: {overall_avg_bleu*100:.2f}")
print(f"Average Token Accuracy: {overall_token_acc*100:.2f}%")

plt.figure(figsize=(12,5))
plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss', marker='o')
plt.plot(range(1, len(val_losses)+1), val_losses, label='Val Loss', marker='o')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss vs. Epochs")
plt.legend()
plt.show()

plt.figure(figsize=(12,5))
plt.plot(range(1, len(bleu_scores)+1), bleu_scores, label='BLEU Score', color='red', marker='o')
plt.xlabel("Epochs")
plt.ylabel("Normalized BLEU Score")
plt.title("BLEU Score vs. Epochs")
plt.legend()
plt.show()

"""# Decoding Functions (Beam Search)"""

def translate_beam_search(model, src_tensor, beam_width=3, max_len=50):
    model.eval()
    src_tensor = src_tensor.unsqueeze(0).to(DEVICE)
    encoder_outputs, hidden, cell = model.encoder(src_tensor)
    beams = [([hindi_vocab['<sos>']], 0.0, hidden, cell)]
    completed = []
    for _ in range(max_len):
        new_beams = []
        for seq, score, hidden, cell in beams:
            if seq[-1] == hindi_vocab['<eos>']:
                completed.append((seq, score))
                continue
            input_token = torch.tensor([seq[-1]], device=DEVICE)
            output, hidden_new, cell_new = model.decoder(input_token, hidden, cell, encoder_outputs)
            log_probs = F.log_softmax(output, dim=1)
            topk_log_probs, topk_indices = log_probs.topk(beam_width)
            for i in range(beam_width):
                new_seq = seq + [topk_indices[0][i].item()]
                new_score = score + topk_log_probs[0][i].item()
                new_beams.append((new_seq, new_score, hidden_new, cell_new))
        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]
        if all(seq[-1] == hindi_vocab['<eos>'] for seq, _, _, _ in beams):
            break
    if not completed:
        completed = beams
    best_seq = max(completed, key=lambda x: x[1])[0]
    return best_seq

def clean_sentence(tokens, vocab):
    words = []
    for idx in tokens:
        word = vocab.lookup_token(idx)
        if word == '<eos>':
            break
        if word not in ['<pad>', '<sos>', '<unk>']:
            words.append(word)
    return " ".join(words)

def display_sample_predictions(model, test_loader, num_samples=5, use_beam=True):
    model.eval()
    samples = []
    with torch.no_grad():
        for src, trg in test_loader:
            src, trg = src.to(DEVICE), trg.to(DEVICE)
            for i in range(min(num_samples, len(src))):
                eng_sentence = clean_sentence(src[i].cpu().numpy(), eng_vocab)
                pred_ids = translate_beam_search(model, src[i]) if use_beam else translate_beam_search(model, src[i], beam_width=1)
                pred_sentence = clean_sentence(pred_ids, hindi_vocab)
                actual_sentence = clean_sentence(trg[i].cpu().numpy(), hindi_vocab)
                samples.append([eng_sentence, pred_sentence, actual_sentence])
                print(f"🟦 ENGLISH   : {eng_sentence}")
                print(f"🟥 PREDICTED : {pred_sentence}")
                print(f"🟩 ACTUAL    : {actual_sentence}")
                print("-" * 80)
            break
    from tabulate import tabulate
    print(tabulate(samples, headers=["English", "Predicted Hindi", "Actual Hindi"], tablefmt="fancy_grid"))

"""Display sample predictions on validation set"""
display_sample_predictions(model, val_loader, num_samples=5, use_beam=True)

"""# Testing on Gold Test Set (Beam Search Decoding)"""

"""Load best model based on validation loss"""
model.load_state_dict(best_model_state)
model.eval()
actuals_gold, predictions_gold = [], []
with torch.no_grad():
    for src, trg in gold_test_loader:
        src, trg = src.to(DEVICE), trg.to(DEVICE)
        output = model(src, trg, teacher_forcing_ratio=0.0)
        pred_tokens = output.argmax(dim=-1).cpu().numpy()
        for i in range(len(src)):
            pred_sentence = [hindi_vocab.lookup_token(idx) for idx in pred_tokens[i] if idx != hindi_vocab['<pad>']]
            actual_sentence = [hindi_vocab.lookup_token(idx) for idx in trg[i].cpu().numpy() if idx != hindi_vocab['<pad>']]
            predictions_gold.append(" ".join(pred_sentence))
            actuals_gold.append([" ".join(actual_sentence)])
gold_bleu = sacrebleu.corpus_bleu(predictions_gold, actuals_gold).score
print(f"\nGold Test BLEU Score: {gold_bleu:.2f}")

"""# For Transfromers

# Hyperparameters
"""

TRANS_EMB_DIM = 300        # Embedding dimension
TRANS_N_LAYERS = 3         # Number of Transformer layers (both encoder and decoder)
TRANS_NHEAD = 6            # Number of attention heads
TRANS_FF_DIM = 512         # Feedforward network hidden dimension
TRANS_DROPOUT = 0.1        # Dropout rate

BATCH_SIZE = 64
NUM_EPOCHS = 10
LR = 0.001                 # Learning rate
WEIGHT_DECAY = 1e-5        # L2 regularization

SPECIAL_TOKENS = ['<unk>', '<pad>', '<sos>', '<eos>']
DATA_FRACTION = 0.70       # Use 70% of data
NUM_FOLDS = 5              # 5-fold cross-validation

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

"""# Data Loading & Preprocessing"""

def load_data(src_file, tgt_file):
    with open(src_file, 'r', encoding='utf-8') as f:
        src_sentences = f.readlines()
    with open(tgt_file, 'r', encoding='utf-8') as f:
        tgt_sentences = f.readlines()
    src_sentences = [s.strip() for s in src_sentences]
    tgt_sentences = [s.strip() for s in tgt_sentences]
    data_size = int(len(src_sentences) * DATA_FRACTION)
    return src_sentences[:data_size], tgt_sentences[:data_size]

src_train, tgt_train = load_data('MT/english.train.txt', 'MT/hindi.train.txt')
src_test, tgt_test = load_data('MT/english.test.txt', 'MT/hindi.test.txt')

src_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')
def tokenize_tgt(text):
    return indic_tokenize.trivial_tokenize_indic(text)
def tokenize(sentences, tokenizer):
    # Add <sos> and <eos> tokens
    return [['<sos>'] + [tok.lower() for tok in tokenizer(sent)] + ['<eos>'] for sent in sentences]
def clean_tgt_tokenizer(text):
    tokens = indic_tokenize.trivial_tokenize_indic(text)
    return [t.lower() for t in tokens if t not in string.punctuation and not re.fullmatch(r'\W+', t)]

src_train_tokens = tokenize(src_train, src_tokenizer)
tgt_train_tokens = tokenize(tgt_train, clean_tgt_tokenizer)
src_test_tokens = tokenize(src_test, src_tokenizer)
tgt_test_tokens = tokenize(tgt_test, clean_tgt_tokenizer)

src_counts = Counter([w for s in src_train_tokens for w in s])
print(src_counts.most_common(20))

"""#Build Vocabularies"""

src_vocab = build_vocab_from_iterator(src_train_tokens, specials=SPECIAL_TOKENS, min_freq=0)
tgt_vocab = build_vocab_from_iterator(tgt_train_tokens, specials=SPECIAL_TOKENS, min_freq=0)
src_vocab.set_default_index(src_vocab['<unk>'])
tgt_vocab.set_default_index(tgt_vocab['<unk>'])

"""# Create Embedding Matrices"""

def create_embedding_matrix(vocab, vectors, embedding_dim=300):
    embedding_matrix = np.zeros((len(vocab), embedding_dim))
    oov_count = 0
    for word, idx in vocab.get_stoi().items():
        try:
            embedding_matrix[idx] = vectors[word]
        except KeyError:
            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))
            oov_count += 1
    print(f"OOV tokens: {oov_count}/{len(vocab)}")
    return torch.from_numpy(embedding_matrix).float()

src_embedding_matrix = create_embedding_matrix(src_vocab, word2vec, TRANS_EMB_DIM)
tgt_embedding_matrix = create_embedding_matrix(tgt_vocab, hindi_vectors, TRANS_EMB_DIM)

"""# Prepare DataLoaders"""

class TransDataset(Dataset):
    def __init__(self, src_data, tgt_data, src_vocab, tgt_vocab):
        self.src_data = src_data
        self.tgt_data = tgt_data
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
    def __len__(self):
        return len(self.src_data)
    def __getitem__(self, idx):
        src_item = torch.tensor([self.src_vocab[token] for token in self.src_data[idx]])
        tgt_item = torch.tensor([self.tgt_vocab[token] for token in self.tgt_data[idx]])
        return src_item, tgt_item

def collate_fn(batch):
    src_batch, tgt_batch = zip(*batch)
    src_batch = pad_sequence(src_batch, padding_value=src_vocab['<pad>'], batch_first=True)
    tgt_batch = pad_sequence(tgt_batch, padding_value=tgt_vocab['<pad>'], batch_first=True)
    return src_batch, tgt_batch

"""# K-Fold split for training/validation using training data"""

kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)
train_idx, val_idx = list(kf.split(src_train))[0]
src_fold_train = [src_train[i] for i in train_idx]
tgt_fold_train = [tgt_train[i] for i in train_idx]
src_fold_val = [src_train[i] for i in val_idx]
tgt_fold_val = [tgt_train[i] for i in val_idx]

src_fold_train_tokens = tokenize(src_fold_train, src_tokenizer)
tgt_fold_train_tokens = tokenize(tgt_fold_train, clean_tgt_tokenizer)
src_fold_val_tokens = tokenize(src_fold_val, src_tokenizer)
tgt_fold_val_tokens = tokenize(tgt_fold_val, clean_tgt_tokenizer)

train_dataset = TransDataset(src_fold_train_tokens, tgt_fold_train_tokens, src_vocab, tgt_vocab)
val_dataset = TransDataset(src_fold_val_tokens, tgt_fold_val_tokens, src_vocab, tgt_vocab)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)

gold_test_dataset = TransDataset(src_test_tokens, tgt_test_tokens, src_vocab, tgt_vocab)
gold_test_loader = DataLoader(gold_test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)

"""# Transformer Model Components"""

class PositionalEncoding(nn.Module):
    def __init__(self, emb_dim, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        pos_enc = torch.zeros(max_len, emb_dim)
        pos = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, emb_dim, 2) * (-math.log(10000.0) / emb_dim))
        pos_enc[:, 0::2] = torch.sin(pos * div_term)
        pos_enc[:, 1::2] = torch.cos(pos * div_term)
        pos_enc = pos_enc.unsqueeze(0)  # shape: [1, max_len, emb_dim]
        self.register_buffer('pos_enc', pos_enc)
    def forward(self, x):
        x = x + self.pos_enc[:, :x.size(1), :]
        return self.dropout(x)

class SourceTransformerEncoder(nn.Module):
    def __init__(self, vocab_size, emb_dim, n_layers, nhead, ff_dim, dropout, pad_idx, pretrained_matrix=None):
        super().__init__()
        self.emb_dim = emb_dim
        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)
        if pretrained_matrix is not None:
            self.embedding.weight.data.copy_(pretrained_matrix)
        self.pos_enc = PositionalEncoding(emb_dim, dropout)
        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dim_feedforward=ff_dim, dropout=dropout, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
    def forward(self, src):
        # src shape: [batch, src_len]
        emb = self.embedding(src) * math.sqrt(self.emb_dim)
        emb = self.pos_enc(emb)
        # Optional: create src_key_padding_mask for padding tokens
        src_key_padding_mask = (src == src_vocab['<pad>'])
        memory = self.encoder(emb, src_key_padding_mask=src_key_padding_mask)
        return memory

class TargetTransformerDecoder(nn.Module):
    def __init__(self, vocab_size, emb_dim, n_layers, nhead, ff_dim, dropout, pad_idx, pretrained_matrix=None):
        super().__init__()
        self.emb_dim = emb_dim
        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)
        if pretrained_matrix is not None:
            self.embedding.weight.data.copy_(pretrained_matrix)
        self.pos_enc = PositionalEncoding(emb_dim, dropout)
        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=nhead, dim_feedforward=ff_dim, dropout=dropout, batch_first=True)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)
        self.fc_out = nn.Linear(emb_dim, vocab_size)
    def forward(self, tgt, memory):
        # tgt shape: [batch, tgt_len]
        emb = self.embedding(tgt) * math.sqrt(self.emb_dim)
        emb = self.pos_enc(emb)
        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)
        tgt_key_padding_mask = (tgt == tgt_vocab['<pad>'])
        output = self.decoder(emb, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask,
                              memory_key_padding_mask=(memory.sum(dim=-1)==0))
        output = self.fc_out(output)
        return output
    def generate_square_subsequent_mask(self, sz):
        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()
        mask = mask.float().masked_fill(mask, float('-inf'))
        return mask

class TransSeq2Seq(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, emb_dim, n_layers, nhead, ff_dim, dropout, device,
                 src_pad_idx, tgt_pad_idx, src_pretrained=None, tgt_pretrained=None):
        super().__init__()
        self.device = device
        self.src_enc = SourceTransformerEncoder(src_vocab_size, emb_dim, n_layers, nhead, ff_dim, dropout, src_pad_idx, pretrained_matrix=src_pretrained)
        self.tgt_dec = TargetTransformerDecoder(tgt_vocab_size, emb_dim, n_layers, nhead, ff_dim, dropout, tgt_pad_idx, pretrained_matrix=tgt_pretrained)
    def forward(self, src, tgt):
        # src: [batch, src_len], tgt: [batch, tgt_len]
        memory = self.src_enc(src)
        output = self.tgt_dec(tgt, memory)
        return output

"""# Instantiate Transformer Model"""

SRC_PAD_IDX = src_vocab['<pad>']
TGT_PAD_IDX = tgt_vocab['<pad>']

INPUT_DIM = len(src_vocab)
OUTPUT_DIM = len(tgt_vocab)
transformer_model = TransSeq2Seq(INPUT_DIM, OUTPUT_DIM, TRANS_EMB_DIM, TRANS_N_LAYERS, TRANS_NHEAD,
                                 TRANS_FF_DIM, TRANS_DROPOUT, DEVICE, SRC_PAD_IDX, TGT_PAD_IDX,
                                 src_pretrained=src_embedding_matrix, tgt_pretrained=tgt_embedding_matrix).to(DEVICE)
print(f"Transformer model is using device: {DEVICE}")

"""# Optimizer and Loss Function"""

optimizer = optim.Adam(transformer_model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
criterion = nn.CrossEntropyLoss(ignore_index=TGT_PAD_IDX)

"""# Training & Evaluation Functions"""

def train_epoch_trans(model, iterator, optimizer, criterion):
    model.train()
    epoch_loss = 0
    for src, tgt in iterator:
        src, tgt = src.to(DEVICE), tgt.to(DEVICE)
        optimizer.zero_grad()
        # For Transformer training, input the target sequence without the last token and compute loss on the rest
        output = model(src, tgt[:, :-1])
        # output shape: [batch, tgt_len-1, vocab_size]
        output_dim = output.shape[-1]
        output = output.contiguous().view(-1, output_dim)
        tgt_out = tgt[:, 1:].contiguous().view(-1)
        loss = criterion(output, tgt_out)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    return epoch_loss / len(iterator)

def evaluate_epoch_trans(model, iterator, criterion):
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for src, tgt in iterator:
            src, tgt = src.to(DEVICE), tgt.to(DEVICE)
            output = model(src, tgt[:, :-1])
            output_dim = output.shape[-1]
            output = output.contiguous().view(-1, output_dim)
            tgt_out = tgt[:, 1:].contiguous().view(-1)
            loss = criterion(output, tgt_out)
            epoch_loss += loss.item()
    return epoch_loss / len(iterator)

def compute_token_accuracy_trans(model, iterator):
    model.eval()
    total_correct = 0
    total_tokens = 0
    with torch.no_grad():
        for src, tgt in iterator:
            src, tgt = src.to(DEVICE), tgt.to(DEVICE)
            output = model(src, tgt[:, :-1])
            preds = output.argmax(dim=-1)
            tgt_out = tgt[:, 1:]
            non_pad = tgt_out != TGT_PAD_IDX
            total_correct += ((preds == tgt_out) & non_pad).sum().item()
            total_tokens += non_pad.sum().item()
    return total_correct / total_tokens if total_tokens > 0 else 0.0

train_losses = []
val_losses = []
bleu_scores = []
token_accuracies = []

best_val_loss = float('inf')
patience = 5
patience_counter = 0
best_model_state = None

for epoch in range(NUM_EPOCHS):
    tr_loss = train_epoch_trans(transformer_model, train_loader, optimizer, criterion)
    val_loss = evaluate_epoch_trans(transformer_model, val_loader, criterion)
    train_losses.append(tr_loss)
    val_losses.append(val_loss)

    # Evaluate BLEU on validation set (corpus-level)
    transformer_model.eval()
    actuals, predictions = [], []
    with torch.no_grad():
        for src, tgt in val_loader:
            src, tgt = src.to(DEVICE), tgt.to(DEVICE)
            output = transformer_model(src, tgt[:, :-1])
            pred_tokens = output.argmax(dim=-1).cpu().numpy()
            for i in range(len(src)):
                # Convert indices to tokens (using the target vocab)
                pred_sentence = [tgt_vocab.lookup_token(idx) for idx in pred_tokens[i] if idx != TGT_PAD_IDX]
                actual_sentence = [tgt_vocab.lookup_token(idx) for idx in tgt[i].cpu().numpy() if idx != TGT_PAD_IDX]
                predictions.append(" ".join(pred_sentence))
                actuals.append([" ".join(actual_sentence)])
    bleu = sacrebleu.corpus_bleu(predictions, actuals).score
    bleu_scores.append(bleu / 100)

    acc = compute_token_accuracy_trans(transformer_model, val_loader)
    token_accuracies.append(acc)

    print(f"Epoch {epoch+1}: Train Loss={tr_loss:.3f}, Val Loss={val_loss:.3f}, BLEU={bleu:.2f}, Token Acc={acc*100:.2f}%")

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_state = transformer_model.state_dict()
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("Early stopping triggered!")
            break

overall_train_loss = np.mean(train_losses)
overall_val_loss = np.mean(val_losses)
overall_avg_bleu = np.mean(bleu_scores)
overall_token_acc = np.mean(token_accuracies)

print("\nOverall Training Metrics:")
print(f"Average Train Loss: {overall_train_loss:.3f}")
print(f"Average Val Loss: {overall_val_loss:.3f}")
print(f"Average BLEU Score: {overall_avg_bleu*100:.2f}")
print(f"Average Token Accuracy: {overall_token_acc*100:.2f}%")

plt.figure(figsize=(12,5))
plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss', marker='o')
plt.plot(range(1, len(val_losses)+1), val_losses, label='Val Loss', marker='o')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss vs. Epochs")
plt.legend()
plt.show()

plt.figure(figsize=(12,5))
plt.plot(range(1, len(bleu_scores)+1), bleu_scores, label='BLEU Score', color='red', marker='o')
plt.xlabel("Epochs")
plt.ylabel("Normalized BLEU Score")
plt.title("BLEU Score vs. Epochs")
plt.legend()
plt.show()

"""# Decoding Functions (Greedy Decoding)"""

def greedy_decode(model, src_tensor, max_len=50):
    model.eval()
    src_tensor = src_tensor.unsqueeze(0).to(DEVICE)  # shape: [1, src_len]
    memory = model.src_enc(src_tensor)
    tgt_indexes = [tgt_vocab['<sos>']]
    for i in range(max_len):
        tgt_tensor = torch.tensor(tgt_indexes, dtype=torch.long, device=DEVICE).unsqueeze(0)
        tgt_mask = model.tgt_dec.generate_square_subsequent_mask(tgt_tensor.size(1)).to(DEVICE)
        output = model.tgt_dec(tgt_tensor, memory)
        next_token = output.argmax(-1)[:, -1].item()
        tgt_indexes.append(next_token)
        if next_token == tgt_vocab['<eos>']:
            break
    return tgt_indexes

def clean_sentence(indices, vocab):
    words = []
    for idx in indices:
        word = vocab.lookup_token(idx)
        if word == '<eos>':
            break
        if word not in ['<pad>', '<sos>', '<unk>']:
            words.append(word)
    return " ".join(words)

def display_sample_predictions_trans(model, test_loader, num_samples=5):
    model.eval()
    samples = []
    with torch.no_grad():
        for src, tgt in test_loader:
            src, tgt = src.to(DEVICE), tgt.to(DEVICE)
            for i in range(min(num_samples, len(src))):
                src_sentence = clean_sentence(src[i].cpu().numpy(), src_vocab)
                pred_ids = greedy_decode(model, src[i])
                pred_sentence = clean_sentence(pred_ids, tgt_vocab)
                actual_sentence = clean_sentence(tgt[i].cpu().numpy(), tgt_vocab)
                samples.append([src_sentence, pred_sentence, actual_sentence])
                print(f"🟦 SRC      : {src_sentence}")
                print(f"🟥 PREDICTED: {pred_sentence}")
                print(f"🟩 ACTUAL   : {actual_sentence}")
                print("-" * 80)
            break
    from tabulate import tabulate
    print(tabulate(samples, headers=["Source", "Predicted Target", "Actual Target"], tablefmt="fancy_grid"))

# Display sample predictions on validation set
display_sample_predictions_trans(transformer_model, val_loader, num_samples=5)

"""# Testing on Gold Test Set (Greedy Decoding)"""

transformer_model.load_state_dict(best_model_state)
transformer_model.eval()
actuals_gold, predictions_gold = [], []
with torch.no_grad():
    for src, tgt in gold_test_loader:
        src, tgt = src.to(DEVICE), tgt.to(DEVICE)
        output = transformer_model(src, tgt[:, :-1])
        pred_tokens = output.argmax(dim=-1).cpu().numpy()
        for i in range(len(src)):
            pred_sentence = [tgt_vocab.lookup_token(idx) for idx in pred_tokens[i] if idx != TGT_PAD_IDX]
            actual_sentence = [tgt_vocab.lookup_token(idx) for idx in tgt[i].cpu().numpy() if idx != TGT_PAD_IDX]
            predictions_gold.append(" ".join(pred_sentence))
            actuals_gold.append([" ".join(actual_sentence)])
gold_bleu = sacrebleu.corpus_bleu(predictions_gold, actuals_gold).score
print(f"\nGold Test BLEU Score: {gold_bleu:.2f}")
